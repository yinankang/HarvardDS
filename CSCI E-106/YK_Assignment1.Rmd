---
title: "Assignment 1"
output: pdf_document
author: "Yinan Kang"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

# Problem 1.42(b)

```{r}
data <- data.frame( x = c(7,12,4,14,25,30), y = c(128,213,75,250,446,540))

llhd.fn <- function(data, b1) {
sol <- 1  
  for (i in 1:nrow(data)) {
    
    sol.temp <- (1/(sqrt(32*pi))) * exp((-1/32)*((data[i,2] - b1*data[i,1]))^2)
    
    sol <- sol*sol.temp
                                    
  }
return(sol)
}

llhd.fn(data, 17)
llhd.fn(data, 18)
llhd.fn(data, 19)
```
Evaluating likelihood function for B1 = 17: 9.45133e-30  
Evaluating likelihood function for B1 = 18: 2.649043e-07  
Evaluating likelihood function for B1 = 19: 3.047285e-37  
  
B1 = 18 results in the largest likelihood function value.

##1.42(c)
```{r}
numer <- 0
for (i in 1:nrow(data)) {
  numer.temp <- data[i,1]*data[i,2]
  numer <- numer + numer.temp
}
denom <- 0
for (i in 1:nrow(data)) {
  denom.temp <- (data[i,1])^2
  denom <- denom+denom.temp
}

b1 <- numer/denom
b1
```
The b1 estimate is: 17.9285. As it rounds to 18, it matches result in (b)  

##1.42(d)
```{r}
results <- c(llhd.fn(data,17),llhd.fn(data,17.5),llhd.fn(data,18),llhd.fn(data,18.5),llhd.fn(data,19))
plot(results,type = "b", xlab = "17, 17.5, 18, 18.5, 19")
```
  

Yes, as is shown in plot above, the likelihood function's maximum is at B1 = 18, consistent with result in (c)  



# Problem 1.43

```{r}
data1 <- read.csv("CDI.csv")
head(data1,3) 
```

```{r}
# Running linear regressions: 

reg.pop <- lm(data1$Number.Active.Physicians ~ data1$Total.Population)
reg.beds <- lm(data1$Number.Active.Physicians ~ data1$Number.of.Hospital.Beds)
reg.pIncome <- lm(data1$Number.Active.Physicians ~ data1$Total.personal.income)

reg.pop
reg.beds
reg.pIncome
```
## 1.43(a)
Regressing when Y = Number of Active Physicians:

Using X = Total Population, Y = -1.106e+02 + 2.795e-03*X  
Using X = Number of Hospital Beds, Y = -95.9322 + 0.7431*X   
Using X = Total Personal Income, Y = -48.3948 + 0.1317*X   

## 1.43(b)

```{r}
plot(data1$Number.Active.Physicians ~ data1$Total.Population)
abline(lm(data1$Number.Active.Physicians ~ data1$Total.Population))
```

```{r}
plot(data1$Number.Active.Physicians ~ data1$Number.of.Hospital.Beds)
abline(lm(data1$Number.Active.Physicians ~ data1$Number.of.Hospital.Beds))
```

```{r}
plot(data1$Number.Active.Physicians ~ data1$Total.personal.income)
abline(lm(data1$Number.Active.Physicians ~ data1$Total.personal.income))
```

From a glance at the plots, yes, a linear regression seems to be a good fit (though it is difficult to see as the outliers "zoom-out" the plot significantly... but there seem to be a cluster of observations near the lower portions of the line, so it seems to be a decent fit).

##1.43(c)
Calculating Mean Squared Error
```{r}
mean(reg.pop$residuals^2)
mean(reg.beds$residuals^2)
mean(reg.pIncome$residuals^2)
```
Assuming no bias, # Hospital beds leads to smallest variability around regression line 