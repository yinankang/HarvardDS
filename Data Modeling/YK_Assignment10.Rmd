---
title: "YK_Assignment10"
author: "Yinan Kang"
date: "4/26/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
require(genridge)
require(lmridge)
require(rpart)
require(boot)
```
# Problem 11.8 

```{r message=FALSE, warning=FALSE}
# Import Data
rm(list=ls())
colnames <- c("y","degree","x3","x4") 
df <- read.table(url("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%2011%20Data%20Sets/CH11PR08.txt"), header=FALSE, sep="", col.names=colnames)
n <- nrow(df) 
attach(df)
```

## (a) Assign X2 and X2 based on 'Degree'
```{r message=FALSE, warning=FALSE}
# Loop to assign X1 and X2
for (i in 1:nrow(df)) {
  if (df$degree[i]==1){
    df$x1[i] <- 0
    df$x2[i] <- 0 
  }
  if (df$degree[i]==2){
    df$x1[i] <- 1
    df$x2[i] <- 0 
  }
  if (df$degree[i]==3){
  df$x1[i] <- 0
  df$x2[i] <- 1 
  }
}
attach(df)
```

## (b) Fit Model, get Regression Plot
```{r}
# Generate model
model <- lm(y~x1+x2+x3+x4)

# Plot residuals
plot(model$fitted.values, model$residuals)
```

**Analysis**: We see that in general, as the 'fitted' values get larger, they also tend to differ more from their corresponding 'actual' values, thus the larger residuals.  

## (c) Groups, and Brown-Forsythe
```{r}
# Add Fitted values to df 
df$fitted <- model$fitted.values

# Order dataframe by ascending 'Fitted' values 
df <- df[order(df$fitted),]

# Make two groups (first 33 in 'gr1', and other 32 in 'gr2') 
gr1 <- df[1:33,]
gr2 <- df[34:nrow(df),]

# DO BROWN FORSYTHE using the 2 groups
d1 <- abs((gr1$fitted-gr1$y)-median(gr1$fitted-gr1$y))  # Eqn 3.8
d2 <- abs((gr2$fitted-gr2$y)-median(gr2$fitted-gr2$y))

# Calculate std. dev from variance
s <- 0
for (k in 1:nrow(gr1)) {
  diff.1 <- (d1[k]-mean(d1))^2
  s <- s+diff.1
}
for (l in 1:nrow(gr2)){
  diff.2 <- (d2[l]-mean(d2))^2
  s <- s+diff.2 
}
s <- s/(n-2) 
s <- sqrt(s) 

# Calculate t-statistic for Brown-Forsythe
t.bf <- (mean(d1)-mean(d2)/(s*sqrt((1/nrow(gr1))+(1/nrow(gr2)))))
t.bf

# Calculate critical t-statistic at alpha=0.01  
t.crit <- qt(1-0.01,n-2) 
```


**Decision Rules**:   
If the absolute value of 't.bf' <= 't.crit', conclude error variance is constant.  
If the absolute value of 't.bf' > 't.crit', conclude error variance is not constant. 

**Conclusion**:  
t.bf = $`r abs(t.bf)`$   
t.crit = $`r t.crit`$  

As 't.bf' > 't.crit', we conclude that error variance is NOT constant between Group 1 and Group 2.  

## (d) Plot absolute residuals with X3 and X4
```{r}
plot(x3,abs(model$residuals))
plot(x4,abs(model$residuals))
```


What do these plots suggest about the relation between the stddev of the error term to X3 and X4?

**Analysis**: We see that there may be a linear relationship between the standard deviation of the error term and X3, due to the vaguely linear pattern we can visually identify. With X4, however, such a relationship is unclear.  

## (e) Estimate standard deviation fn, calculate estimated weights 
```{r}
# Capturing residuals
ei <- model$residuals
abs.ei <- abs(ei) 

# Generating standard deviation functions against X3 and X4
model.1 <- lm(abs.ei~df$x3+df$x4)
s.1 <- model.1$fitted.values

# Below are the estimated weights for each case in the model:  
wi <- 1/(s.1^2) 
print(wi)
```

*The above table shows the individual weights for each case* 


## (f) Obtain weighted least squares fit

```{r}
model.w <- lm(y~df$x1+df$x2+df$x3+df$x4, weights=wi)

model
model.w
```


Are the WLS estimates of regression coefficients similar to the ones obtained with OLS?  

**Analysis**: No, the coefficients are significantly different. 


## (g) Compare estimated standard deviation of the WLS coefficients in (f) with the OLS ones in (b)  
```{r}
# The t-value of the coefficients, called by 'summary(model)', is an estimate 
# of the standard deviation of the estimate.  

summary(model)
summary(model.w) 
```


What do you find?  

**Analysis**: We find that the standard deviation estimates of the coefficients in the two models are precisely the same.   


## (h) Iterate steps (e)-(f) one more time
```{r}
# Generate 2nd standard deviation function, weights, and model
model.2 <- lm(abs(model.w$residuals)~df$x3 + df$x4) 
s.2 <- model.2$fitted.values

# Below are the estimated weights for each case in the model:  
wi.2 <- 1/(s.2^2) 
print(wi.2)

model.w2 <- lm(y~df$x1+df$x2+df$x3+df$x4, weights=wi.2)

# Compare model coefficients
model.w
model.w2
```


Is there substantial change in the coefficients?

**Analysis**: Yes, there is substantial change, especially for X4.  


# Problem 11.23 - Cement Composition
```{r message=FALSE, warning=FALSE} 
# Import data
rm(list=ls())
colnames <- c("y","x1","x2","x3","x4") 
df <- read.table(url("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%2011%20Data%20Sets/CH11PR23.txt"), header=FALSE, sep="", col.names=colnames)
n <- nrow(df) 
attach(df)
```

## (a) Fit regression, state function

```{r}
model <- lm(y~x1+x2+x3+x4)
model 
```


**Model**: 62.4054 + 1.5511*X1 + 0.5102*X2 + 0.1019*X3 - 0.1441*X4  


## (b) Obtain estimated ridge standardized regression coefficients, VIF, R^2
```{r}
lambda1 <- c(.000,.002,.004,.006,.008,.02,.04,.06,.08,.1) 
model.r <- ridge(y,as.matrix(df[,2:5]),lambda=lambda1)

# Coefficients of ridge 
coef(model.r)

# Variance inflation factors
vif(model.r)

# R^2 
## Using 'lmridge' package to calculate R^2 values ## 

model.r2 <- lmridge(y~x1+x2+x3+x4, data=df, K=lambda1)
rstats1(model.r2) # provides R2 and adj-R2 values
```

## (c) Make ridge trace plot
```{r}
traceplot(model.r)
```


Do the ridge regression coedfficients exhibit substantial changes near c=0?  

**Analysis**: Around c=0, the ridge coefficients DO exhibit substantial changes.  


## (d) Suggest reasonable 'c' based on ridge trace, VIF, and R-squared values

**Response**: I would choose **c = 0.1** as the most appropriate. This is the 'c' value at which the VIF values are most near 1, and where the ridge trace lines stabilize the most. The trade-off is in the R-squared values, as c=0.1 has the lowest R-squared value. However, and adj-R2 = 0.8552, it is still a good value overall.   


## (e) Transform ridge regression coefficients back to original variables  
```{r}
# Using formulas 7.43c-d and 7.53a-b
## Calculating necessary values

## s values according to 7.43c-d
syy <- 0
for (i in 1:nrow(df)) {
  y.temp <- (y[i] - mean(y))^2
  syy <- syy + y.temp
}

sx1 <- 0
for (i in 1:nrow(df)) {
  x1.temp <- (x1[i] - mean(x1))^2
  sx1 <- sx1 + x1.temp
}
sx2 <- 0
for (i in 1:nrow(df)) {
  x2.temp <- (x2[i] - mean(x2))^2
  sx2 <- sx2 + x2.temp
}
sx3 <- 0
for (i in 1:nrow(df)) {
  x3.temp <- (x3[i] - mean(x3))^2
  sx3 <- sx3 + x3.temp
}
sx4 <- 0
for (i in 1:nrow(df)) {
  x4.temp <- (x4[i] - mean(x4))^2
  sx4 <- sx4 + x4.temp
}

sy <- sqrt(syy/(n-1)) 
s1 <- sqrt(sx1/(n-1)) 
s2 <- sqrt(sx2/(n-1)) 
s3 <- sqrt(sx3/(n-1)) 
s4 <- sqrt(sx4/(n-1)) 

# Calculating new TRANSFORMED coefficients using 7.53a-b
b1.new <- (sy/s1)*model.r$coef[1]
b2.new <- (sy/s2)*model.r$coef[2]
b3.new <- (sy/s3)*model.r$coef[3]
b4.new <- (sy/s4)*model.r$coef[4]

b0.new <- mean(y) - b1.new*mean(x1) - b2.new*mean(x2) - b3.new*mean(x3) - b4.new*mean(x4)   #7.53b

# Creating results vector 
new.fits <- as.vector(c(rep(0,13)))

# Looping through 'df' to calculate new fits and saving in 'new.fits'
for (j in 1:nrow(df)) {
  fit <- b0.new + b1.new*x1[j] + b2.new*x2[j] + b3.new*x3[j] + b4.new*x4[j]
  new.fits[j] <- fit 
}

# Compare transformed fits to original fits 
new.fits
model$fitted.values

mean(abs(new.fits-model$fitted.values))

plot(new.fits)
plot(model$fitted.values)
```


**Analysis**: After transforming the ridge regression coefficients back onto the original variables, we can analyze the difference in fitted values. Numerically, the mean absolute difference is $`r mean(abs(new.fits-model$fitted.values))`$, while visually when we plot the two sets, we can notice that the original fitted values reach a max of about 115, while the new fitted values climb has high as 150, and even 200. The patterns of increase differ as well.  


# Problem 11.28 - Mileage Study
```{r message=FALSE, warning=FALSE}
rm(list=ls())
colnames <- c("y","x") 
df <- read.table(url("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%2011%20Data%20Sets/CH11PR28.txt"), header=FALSE, sep="", col.names=colnames)
n <- nrow(df) 
attach(df)
```

## (a) Fit 2nd-order regression model
```{r}
model <- lm(y~poly(x,2))
model
plot(x,y)
curve(predict(model,newdata=data.frame(x=x)),add=T)
```



**Analysis**: Looking at the plot, YES the 2nd-order function looks to be a good fit.  


## (b) Estimate Xmax 
```{r}
x.hat.max <- mean(x) - (0.5*9.804/(-19.674)) 
y.hat <- 32.000 + x.hat.max*9.804 + (x.hat.max^2)*(-19.674)
```


**Results**: The estimated speed Xmax is $`r x.hat.max`$ and the mean milage at that speed is $`r y.hat`$.  


## (c) Bootstrap sampling
```{r}
# Using 'boot' package and 'boot()' function to create bootstrap sampling

# Defining function to be passed into 'boot()' function (returning the X.max.hat) 
xmaxfn <- function(formula, data, indices) {
  d <- data[indices,] # allows boot to select sample 
  fit <- lm(formula, data=d)
  x.hat.max.boot <- mean(x) - (0.5*fit$coefficients[2]/(fit$coefficients[3])) 
  return(x.hat.max.boot)
} 
# bootstrapping with 1000 replications 
results <- boot(data=df, statistic=xmaxfn, 
   R=1000, formula=y~poly(x,2))

# Histogram of bootstrap sample results
hist(results)
```


**Analysis**: We see from the histogram that YES, the results of the X-max-hat bootstrap sampling appear to be Normal.  


# Problem 11.30 - Patient Satisfaction
```{r message=FALSE, warning=FALSE}
rm(list=ls())
colnames <- c("y","x1","x2","x3") 
df <- read.table(url("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%206%20Data%20Sets/CH06PR15.txt"), header=FALSE, sep="", col.names=colnames)
n <- nrow(df) 
attach(df)
```

## Fit two-region regression tree
```{r}
tree <- rpart(y~x1+x2) # By default, the tree created only had 2 splits, and since the book questions asked about 4 splits...
tree <- rpart(y~x1+x2,control=list(minsplit=1,cp=.000001))  # I Manipulated the cp value to generate more splits
tree
```

## (a) First split point is based on X1, at X1 = 36.5, SSE = 13369.3000  

## (b) Second split point is based on X1, at X1 = 46.10, SSE = 713.600  
 
## (c) Third split point is based on X1, at X1 = 29.5, SSE = 658.400  

## (d) Fourth split point is based on X1, at X1 = 42.5, SSE = 567.4286  

## (e) Plot tree
```{r}
plot(tree)
text(tree)

```


**Analysis**: Judging by this tree, we would say that X1 appears to be relatively more important than X2.  


## (f) Residual Plot
```{r}
# Generate fitted values of Tree
pred <- predict(tree)
ei <- pred-y
plot(ei)
```


**Analysis**: We see that the residuals center around 0, but have relatively dramatic swings both up and down (a residual of 20 is large for the context of this 'y' value).  

# Problem 11.31 - Prostate cancer
```{r message=FALSE, warning=FALSE}
# Import data
rm(list=ls())
colnames <- c("id","y","x1","x2","x3","x4","x5","x6","x7") 
df <- read.table(url("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Appendix%20C%20Data%20Sets/APPENC05.txt"), header=FALSE, sep="", col.names=colnames)
df$id <- NULL # Take out 'id' column as it's not a predictor
n <- nrow(df) 
attach(df)
```

## (a) Sample and Fit Tree
```{r}
set.seed(5)
train.rows <- sample(nrow(df), 65,replace=FALSE)
train <- df[train.rows,]
test <- df[-train.rows,]

tree <- rpart(y ~ x1+x2+x3+x4+x5+x6+x7,data=train)
tree
```

**Justification**: The default rpart() tree is based on recommended control considerations (i.e. ratios between 'minsplit','cp','maxdepth',etc.) thus is more or less acceptable for use here.  


## (b) Evaluate predictive capability
```{r}
# Using 'test' or remainder of the observations to evaluate predictive capability  
pred <- predict(tree, newdata=test)

# Calculate mean squared error as a metric, and also mae as a % of the PSA value
mae <- mean(abs(pred-test$y)) 
mae.percentage <- mae/test$y*100
mae.percentage
```


**Analysis**: We see that the 'mae' value is $`r mae`$, which for the majority of the observations, is substatial relative to 'y' or 'PSA' value. This is supported by the vector 'mae.percentage', which prints out the percentage error for each observaion in the test set. We see that it regularly is extremely high - in fact, only in 1 case is it acceptable $`r mae.percentage[length(mae.percentage)]`$ %.    

Thus, I would conclude this model would be very poor for predictive use by doctors.  


## (c) Compare with previous model

Case study 9.30 was never assigned. Thus, I will compare the above Tree model with a simple OLS regression.  


````{r}
model.comp <- lm(y~.,data=train) 
pred.comp <- predict(model.comp, newdata=test)
mae.comp <- mean(abs(pred.comp-test$y))
mae.percentage.comp <- mae.comp/test$y*100
mae.percentage.comp
```

**Comparison**: We see that the tree model is slightly worse than the standard OLS regression model, although it is quite slight. The OLS model has a slightly better MAE, but as we see from the error percentages, they are both very (very) high for many cases.   

