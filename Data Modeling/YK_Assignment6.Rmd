---
title: "YK_Assignment6"
author: "Yinan Kang"
date: "3/30/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Note: Hi TAs, I skipped a few sections, you will find 'n/a' next to them. Had a bit less time this week thanks to work (and a trip to see my parents). Still learned a lot from doing this assignment, though this will probably be my 'drop'. Thx -Yinan 


```{r message= FALSE, warning= FALSE}
# Packages

install.packages("car")
install.packages("corrplot")
require(car)
require(corrplot)
require(MASS)
```
# Problem 5.26
## (a) 
```{r}
# Import Data
colnames <- c("y","x") 
df <- read.table(url("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%201%20Data%20Sets/CH01PR22.txt"), header=FALSE, sep="", col.names=colnames)
n <- nrow(df) 

X = matrix(c(rep(1,16),df$x[1],df$x[2],df$x[3],df$x[4],df$x[5],df$x[6],df$x[7],df$x[8],df$x[9],df$x[10],df$x[11]
             ,df$x[12],df$x[13],df$x[14],df$x[15],df$x[16]),nrow=16,ncol=2)
Y = matrix(c(df$y[1],df$y[2],df$y[3],df$y[4],df$y[5],df$y[6],df$y[7],df$y[8],df$y[9],df$y[10],df$y[11]
             ,df$y[12],df$y[13],df$y[14],df$y[15],df$y[16]), nrow=16, ncol=1)

# Calculations
# (1) (X'X)^-1
ginv(t(X) %*% X)

# (2) b
b<- (ginv(t(X) %*% X)) %*% (t(X)%*%Y)
b

# (3) Y-hat
h = X%*%(ginv(t(X) %*% X))%*%t(X)
h %*% Y # Y-hat answer

# (4) H
h

# (5) SSE
t(Y-X%*%b)%*%(Y-X%*%b)

# (6) s2{b}
## calculate MSE
model <- lm(y~x, data=df)
mse <- mean(model$residuals^2) 

mse*(ginv(t(X) %*% X)) # s2{b} 

# (7) s2{pred} when Xh = 30
xh <- matrix(c(1,30),nrow=2,ncol=1)
mse*(1+t(xh)%*%(ginv(t(X) %*% X))%*%xh) #s2{pred} answer
```
## (b) n/a 
## (c) n/a
```{r}


```

# Problem 6.18
## (a)
```{r}
rm(list=ls())
# Import Data
colnames <- c("y","x1","x2","x3","x4") 
df <- read.table(url("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%206%20Data%20Sets/CH06PR18.txt"), header=FALSE, sep="", col.names=colnames)
n <- nrow(df) 

# Stem and Leaf for each X: 
stem(df$x1)
stem(df$x2)
stem(df$x3)
stem(df$x4)


```
What can be learned from these stem and leaf plots? 

Response: We see the distributions of the different X values. X2 and X4 are largely normally distributed, with slight skews (right for X2, left for X4). X1 looks to have a bimodal distribution and X3's observations lie mostly in the 0-10 range, with only a few greater than 20s. 

## (b)
```{r}
# Scatterplot 
pairs(y~x1+x2+x3+x4, data=df)

# Correlation Plot
m <- cor(df)
m
corrplot(m)

```

Interpretation: We see that the predictors are not too correlated, with the highest correlation being ~0.5 between X1 and X4.   

## (c)
```{r}
# Fit regression model
model <- lm(y~x1+x2+x3+x4, data=df)
```
Regression model: y = 1.22e+01 - 1.42e-01(x1) + 2.82e-01(x2) + 6.19e-01(x3) + 7.92e-06(x4)  

## (d)
```{r}
# Box-plot of residuals
boxplot(model$residuals)

```

Does the distribution of residuals seem fairly symmetrical?  

Response: Yes, it does based on boxplot. 

## (e) 
```{r}
# Plot Residuals vs. Fitted Values
plot(model$residuals ~ model$fitted.values)

# Residuals vs. X1,X2,X3,X4
par(mfrow=c(2,2))
plot(model$residuals ~ df$x1)
plot(model$residuals ~ df$x2)
plot(model$residuals ~ df$x3)
plot(model$residuals ~ df$x4) 

## Normal Probability Plot 
model.stan <- rstandard(model)
qqnorm(model.stan, main="Normal Q-Q Plot", xlab="Expected", ylab="Standardized residual")
qqline(model.stan)

```

Interpretation: The residuals look approximately normal and the distribution does not look to have a pattern.  


## (f) 
Can a formal lack of fit test be conducted here?  

Response: Yes, because there are multiple samples for each X, F-test can be performed here.  

## (g) n/a


# Problem 6.19 
## (a)
Hypothesis Decision:  
H0: B1 = B2 = ... = 0  
Ha: not all B values (k=1,...,p-1) equal 0    

If f.crit <= f ... conclude H0   
If f.crit > f ... conclude Ha   

```{r}
# Calculate SSR
anova(model)
SSR <- 14.819+72.802+8.381+42.325 
msr <- SSR
mse <- mean(model$residuals^2) 

# Calculate F values
f.crit <- msr/mse
p <- 4
n <- n
f <- qf(.95,p-1,n-p)

print(f.crit)
print(f) 
```
Hypothesis Decision:  
H0: B1 = B2 = ... = 0  
Ha: not all B values (k=1,...,p-1) equal 0    

If f.crit <= f ... conclude H0   
If f.crit > f ... conclude Ha   

Decision: Since f.crit > f, we conclude Ha, that there IS a regression relation among the 4 predictors. Also, p value is 7.272e-14.   

## (b) n/a 

## (c) 
R^2 = 0.5847, from summary(model). The multivariate linear regression model we've built only accounts for roughly just greater than half the variance existing in the data, which shows it is likely not a good-fitting model.   

# Problm 6.28  
```{r message=FALSE, warning=FALSE}
# Importing Data
rm(list=ls())
cdi.df <- read.csv("CDI.csv") 
n <- nrow(cdi.df)
attach(cdi.df) 

# Model 1 predictors 
x1 <- Total.Population
x2 <- Land.Area
x3 <- Total.personal.income

# Model 2 predictors 
x4 <- Total.Population / Land.Area
x5 <- X.Pop.aged.65.and.over/Total.Population
x6 <- Total.personal.income

Y <- Number.Active.Physicians
```

## (a)
```{r}
# Stem and leaf plots
par(mfrow=c(3,2))
stem(x1)
stem(x2)
stem(x3)
stem(x4)
stem(x5)
stem(x6) 

```
Noteworthy takeaway here is that for each predictor, values skew to the low end.  

## (b) 
```{r}
# Generating models 
model1 <- lm(Y~x1+x2+x3)
model2 <- lm(Y~x4+x5+x6) 

# Scatterplot s
pairs(Y~x1+x2+x3)
pairs(Y~x4+x5+x6)

# Correlation Plot
cor1 <- cor(data.frame(Y,x1,x2,x3))
cor2 <- cor(data.frame(Y,x4,x5,x6))
corrplot(cor1)
corrplot(cor2) 
```
Findings:  

From scatterplot, we see close to linear relationship between personal income (x3,x6) and Y and also total population (x1) and Y. 

From correlation matrix, for model 1, we see that total population and personal income (x1 and x3) are heavily correlated together and with Y. Form model 2, personal income (x6) heavily correlates with Y.  

## (c) Fit model, already done in (b) 

## (d) 
```{r}
summary(model1)
summary(model2) 

```
Conclusion:  
Model 2 produces the higher R^2 value, thus explaining more variation than Model 1.  

## (e)
```{r}
# Obtaining residuals:  
res1 <- model1$residuals
res2 <- model2$residuals

# Residual plots: 
par(mfrow=c(1,2))
plot(res1~model1$fitted.values)
plot(res2~model2$fitted.values)

par(mfrow=c(3,2))
plot(res1~x1)
plot(res1~x2)
plot(res1~x3)
plot(res2~x4)
plot(res2~x5)
plot(res2~x6) 

# Normal Probability Plots 

par(mfrow=c(1,2))
model1.stan <- rstandard(model1)
model2.stan <- rstandard(model2)
qqnorm(model1.stan, main="Model1", xlab="Expected", ylab="Standardized residual")
qqline(model1.stan)
qqnorm(model2.stan, main="Model2", xlab="Expected", ylab="Standardized residual")
qqline(model2.stan)
```

Analysis: We can see that the residuals for Model 2 are more well-behaved than Model 1. Residuals are closer to 0 for more of its predictors, and the Q-Q plot shows more normality than for Model 1.  

## 7.7 n/a 